**File:** `mediation-ddls\mnaas_a2p_p2a_sms_audit_raw.hql`  

---

### 1. High‑Level Summary
This Hive DDL script creates the **`mnaas.a2p_p2a_sms_audit_raw`** table, a transactional, insert‑only, partition‑by‑date Parquet store for raw SMS‑audit records generated by the A2P/P2A messaging platform. The table is intended to be the landing zone for raw audit files (CSV/JSON/etc.) before downstream cleansing, enrichment, and reporting pipelines consume it.

---

### 2. Core Objects Defined
| Object | Type | Responsibility |
|--------|------|-----------------|
| `mnaas.a2p_p2a_sms_audit_raw` | Hive/Impala table | Holds raw audit rows with fields such as `filename`, `id`, `typeofsms`, timestamps (`smsrequestdate`, `smssenton`, `smsdeliveredon`), subscriber identifiers (`msisdn`, `iccid`), product and MVNO metadata, and audit status fields. |
| `partition_date` | Partition column (string) | Enables daily partitioning for incremental loads and pruning. |
| Table properties (e.g., `transactional='true'`, `insert_only`) | Hive metadata | Guarantees ACID‑compatible inserts while preventing updates/deletes, simplifying ingestion pipelines. |
| Storage format | Parquet via Hive SerDe | Provides columnar compression and efficient scan performance for downstream analytics. |
| HDFS location | `hdfs://NN-HA1/.../a2p_p2a_sms_audit_raw` | Physical storage path for the table data. |

*No procedural code (functions, classes) is present; the file is pure DDL.*

---

### 3. Inputs, Outputs & Side Effects
| Aspect | Details |
|--------|---------|
| **Inputs** | External ingestion jobs (e.g., Spark, Sqoop, custom ETL) that write raw audit files into the table using `INSERT INTO … PARTITION (partition_date='YYYY-MM-DD')`. The source files may arrive via SFTP, Kafka, or an internal messaging bus. |
| **Outputs** | The populated Hive/Impala table, queried by downstream transformation scripts (e.g., `mnaas_*_sms_*_transformation.hql`), reporting jobs, or BI tools. |
| **Side Effects** | • Creates the table metadata in the Hive Metastore.<br>• Instantiates the HDFS directory hierarchy under the specified location.<br>• Registers the table with Impala catalog (via the `impala.events.*` properties). |
| **Assumptions** | • Hive/Impala cluster is reachable and the `mnaas` database exists.<br>• HDFS namenode alias `NN-HA1` resolves and the service account has write permission to the target path.<br>• Partition column values are supplied as `YYYY-MM-DD` strings by upstream jobs.<br>• No updates/deletes are required (insert‑only model). |

---

### 4. Integration Points with Other Scripts / Components
| Connected Component | Interaction |
|---------------------|-------------|
| **Ingestion pipelines** (e.g., `mnaas_a2p_p2a_sms_ingest.py`, Spark jobs) | Perform `INSERT INTO mnaas.a2p_p2a_sms_audit_raw PARTITION (partition_date=…) SELECT …` from raw files. |
| **Transformation scripts** (e.g., `mnaas_a2p_p2a_sms_enrich.hql`) | Read from this table, apply cleansing, and write to curated tables (`mnaas.a2p_p2a_sms_audit_curated`). |
| **Reporting jobs** (e.g., `market_management_report_sims.hql` from history) | Join on `msisdn`, `productid`, `mvnoid` to produce KPI dashboards. |
| **Metadata/Orchestration** (Airflow, Oozie) | The DDL is executed as a “create‑if‑not‑exists” task before any load DAG runs. |
| **Impala UI / BI tools** | Query the table directly for ad‑hoc analysis. |
| **Stat collection jobs** (e.g., `run_compute_stats.sh`) | Periodically run `ANALYZE TABLE … COMPUTE STATISTICS` because `DO_NOT_UPDATE_STATS='true'` disables automatic stats. |

---

### 5. Operational Risks & Recommended Mitigations
| Risk | Mitigation |
|------|------------|
| **Unbounded partition growth** – using a string partition may lead to many small partitions if dates are not normalized. | Enforce strict `YYYY-MM-DD` format in upstream jobs; schedule a daily “drop old partitions” job after retention period. |
| **Missing statistics** – `DO_NOT_UPDATE_STATS='true'` can cause sub‑optimal query plans. | Add a nightly `ANALYZE TABLE mnaas.a2p_p2a_sms_audit_raw PARTITION (partition_date) COMPUTE STATISTICS` step. |
| **Storage bloat** – insert‑only tables never purge data. | Implement a retention policy (e.g., drop partitions older than 90 days) and monitor HDFS usage. |
| **Schema drift** – all columns are `string`; downstream jobs may expect timestamps or numeric types. | Add a schema‑validation step in the ingestion pipeline; consider altering column types to `timestamp`/`bigint` where appropriate. |
| **Permission issues** – HDFS path may be inaccessible to new service accounts. | Document required ACLs (`rw` for the ETL service principal) and include a pre‑deployment checklist. |

---

### 6. Running / Debugging the Script
1. **Execute**  
   ```bash
   hive -f mediation-ddls/mnaas_a2p_p2a_sms_audit_raw.hql
   ```
   - Or, within an Airflow DAG: `HiveOperator(task_id='create_audit_raw', hql='{{ var.value.mnaas_a2p_p2a_sms_audit_raw }}')`.

2. **Verify Creation**  
   ```sql
   SHOW CREATE TABLE mnaas.a2p_p2a_sms_audit_raw;
   DESCRIBE FORMATTED mnaas.a2p_p2a_sms_audit_raw;
   ```

3. **Check HDFS Directory**  
   ```bash
   hdfs dfs -ls /warehouse/tablespace/managed/hive/mnaas.db/a2p_p2a_sms_audit_raw
   ```

4. **Debug Common Issues**  
   - *Metastore error*: ensure the `mnaas` database exists (`CREATE DATABASE IF NOT EXISTS mnaas;`).  
   - *Permission denied*: verify the executing user has write ACL on the target HDFS path.  
   - *Duplicate table*: drop with `DROP TABLE IF EXISTS mnaas.a2p_p2a_sms_audit_raw;` before re‑run (only in dev).  

---

### 7. External Config / Environment Variables
| Variable / Config | Usage |
|-------------------|-------|
| `HIVE_CONF_DIR` / `HIVE_HOME` | Hive client configuration for locating metastore URI. |
| `NN-HA1` (namenode alias) | Resolved by the cluster’s DNS; points to the HDFS HA namenode. |
| `WAREHOUSE_DIR` (optional) | If overridden, changes the base path for the `LOCATION` clause. |
| `IMPALA_CATALOG_SERVICE_ID` / `IMPALA_CATALOG_VERSION` | Populated automatically; no manual setting required. |
| **Potential external files**: a properties file (`hive-site.xml`) that defines `hive.metastore.uris`, `hdfs.client.failover.proxy.provider`, etc. |

---

### 8. Suggested TODO / Improvements
1. **Data‑type refinement** – Change `smsrequestdate`, `smssenton`, `smsdeliveredon` from `string` to `timestamp` (or `bigint` epoch) to enable time‑based predicates and reduce storage overhead.  
2. **Partitioning strategy** – Convert `partition_date` to a `date` type and consider adding a secondary partition on `mvnoid` if query patterns frequently filter by MVNO, to improve pruning.  

---