**File:** `mediation-ddls\mnaas_ra_early_deplition_prediction.hql`  

---

## 1. High‑Level Summary
This Hive DDL script creates the **`ra_early_deplition_prediction`** table in the `mnaas` database. The table stores per‑device early‑depletion forecasts generated by a predictive model (e.g., “will the data bundle run out early?”). Each row contains device‑level identifiers, customer and cost‑center metadata, the forecasted data‑bundle parameters, usage statistics, and a calculated “predicted data‑balance‑days” value. The table is **partitioned by `date_of_run`** (the execution date of the prediction job) and is defined as an **insert‑only transactional Hive table** to support atomic bulk loads from downstream Spark/MapReduce jobs.

---

## 2. Core Objects Defined in the Script  

| Object | Type | Responsibility / Description |
|--------|------|--------------------------------|
| `ra_early_deplition_prediction` | Hive Managed Table | Holds early‑depletion prediction records for every device (identified by `tcl_secs_id`, `iccid`, `msisdn`, `imsi`). |
| `date_of_run` | Partition column (string) | Enables efficient pruning of data by the run date of the prediction batch. |
| Table properties (`transactional='true'`, `transactional_properties='insert_only'`) | Hive metadata | Guarantees ACID‑style bulk inserts while preventing updates/deletes (fits typical “append‑only” prediction loads). |
| SerDe (`LazySimpleSerDe`) & format properties | Hive I/O configuration | Reads/writes CSV‑style text files with comma delimiter and newline line terminator. |
| `LOCATION` | HDFS path | Physical storage location: `hdfs://NN-HA1/warehouse/tablespace/managed/hive/mnaas.db/ra_early_deplition_prediction`. |

*No procedural code (functions, procedures, or UDFs) is defined in this file; it is purely a DDL definition.*

---

## 3. Inputs, Outputs, Side‑Effects & Assumptions  

| Category | Details |
|----------|---------|
| **Inputs** | None at DDL execution time. The table will later be populated by downstream jobs (e.g., Spark, Hive INSERT‑OVERWRITE, or Sqoop). |
| **Outputs** | A Hive managed table stored as text files under the specified HDFS location. The table becomes visible to any Hive/Presto/Trino query engine that can read the `mnaas` database. |
| **Side‑effects** | - Creates a new directory in HDFS (if it does not exist). <br> - Registers metadata in the Hive metastore. |
| **Assumptions** | - Hive metastore is reachable and the user has `CREATE` privileges on the `mnaas` database. <br> - HDFS namenode `NN-HA1` is reachable and the path is writable. <br> - Downstream jobs will respect the `date_of_run` partitioning scheme. <br> - All columns are stored as `STRING`; downstream consumers will cast to appropriate types when needed. |

---

## 4. Integration Points (How This Table Connects to the Rest of the System)

| Connected Component | Relationship |
|---------------------|--------------|
| **Prediction Engine** (e.g., Spark job `early_depletion_predictor.py`) | Writes bulk CSV/TSV files to a staging HDFS directory, then issues `INSERT INTO mnaas.ra_early_deplition_prediction PARTITION (date_of_run='YYYYMMDD') SELECT …` to load data. |
| **Mediation DDL Suite** (other `mnaas_ra_*.hql` files) | Shares the same `mnaas` database and similar partitioning strategy (`date_of_run`). Downstream reporting tables may join on `tcl_secs_id` or `iccid`. |
| **Reporting / BI Layer** (e.g., Tableau, PowerBI, Superset) | Queries the table for “early‑depletion alerts” dashboards; may filter on `prediction_month` or `predicted_data_balance_days`. |
| **Alerting Service** (e.g., Kafka producer) | Consumes rows where `predicted_data_balance_days < threshold` and pushes notifications to customer‑care or automated SMS. |
| **Data Governance / Auditing** | The `insert_time_stamp` column is used by audit jobs to verify load freshness. |
| **Retention / Archival Jobs** | Periodic Hive scripts truncate or move older partitions (`date_of_run` < N days) to an archive location. |

*Because the table is insert‑only, any correction to predictions must be performed by creating a new partition (new run date) rather than updating existing rows.*

---

## 5. Operational Risks & Recommended Mitigations  

| Risk | Impact | Mitigation |
|------|--------|------------|
| **All columns are `STRING`** – numeric/date values stored as text can cause parsing errors downstream. | Data quality issues, increased storage, slower query performance. | Add a schema review step; consider altering column types to `BIGINT`, `DOUBLE`, `DATE` where appropriate. |
| **Partition key is a `STRING`** – may lead to many small partitions if the format is inconsistent. | Metastore bloat, query planner slowdown. | Enforce a strict `YYYYMMDD` format via validation in the loading job. |
| **Insert‑only transactional table** – cannot perform updates or deletes. | Stale or erroneous predictions cannot be corrected without a new run. | Design the downstream pipeline to always recompute predictions for the same `date_of_run` (overwrite partition) or add a “reprocess” flag. |
| **Hard‑coded HDFS location** – changes in cluster topology (e.g., namenode rename) break the table. | Table becomes unreadable. | Externalize the base HDFS path to a configuration file (e.g., `hive-site.xml` property `mnaas.base.path`). |
| **No column comments or documentation** – future developers may misinterpret fields. | Maintenance overhead, risk of misuse. | Add `COMMENT` clauses to each column; store a data‑dictionary in a separate metadata table. |

---

## 6. Running / Debugging the Script  

1. **Prerequisites**  
   - Hive client (`beeline` or `hive`) installed on the operator workstation or on the edge node.  
   - Access to the Hive metastore with `CREATE` rights on `mnaas`.  
   - HDFS user has write permission on the target location.  

2. **Execution Command**  
   ```bash
   # Using Beeline (recommended for Kerberos‑enabled clusters)
   beeline -u "jdbc:hive2://<hive-server-host>:10000/mnaas;principal=hive/_HOST@REALM" \
           -f mediation-ddls/mnaas_ra_early_deplition_prediction.hql
   ```
   *Or, if using the legacy Hive CLI:*  
   ```bash
   hive -f mediation-ddls/mnaas_ra_early_deplition_prediction.hql
   ```

3. **Verification Steps**  
   - After execution, run:  
     ```sql
     SHOW CREATE TABLE mnaas.ra_early_deplition_prediction;
     DESCRIBE FORMATTED mnaas.ra_early_deplition_prediction;
     ```  
   - Confirm that the HDFS directory exists and contains a `_metadata` file:  
     ```bash
     hdfs dfs -ls /warehouse/tablespace/managed/hive/mnaas.db/ra_early_deplition_prediction
     ```  

4. **Debugging Tips**  
   - **Syntax errors**: Hive will report line numbers; fix in the `.hql` file and re‑run.  
   - **Permission errors**: Check HDFS ACLs (`hdfs dfs -getfacl …`) and Hive metastore role.  
   - **Metastore conflicts**: If a table with the same name already exists, either drop it (`DROP TABLE IF EXISTS mnaas.ra_early_deplition_prediction;`) or rename the new table.  
   - **Partition visibility**: After loading data, run `MSCK REPAIR TABLE mnaas.ra_early_deplition_prediction;` to refresh partitions if external loading was used.  

---

## 7. External Configuration / Environment Variables Referenced  

| Config Item | Usage |
|-------------|-------|
| `hdfs://NN-HA1/...` (namenode URI) | Hard‑coded in the `LOCATION` clause; determines where the table data lives. |
| Hive metastore connection (e.g., `hive.metastore.uris`) | Not in the script but required for any Hive client that runs the DDL. |
| Kerberos principal / keytab (if security enabled) | Needed for `beeline` connection; not referenced directly in the script. |
| Optional: `mnaas.base.path` (if introduced later) | Could replace the hard‑coded HDFS path to make the script portable across environments (dev, test, prod). |

*If the organization uses a parameter‑substitution framework (e.g., `${HDFS_ROOT}`), the script currently does **not** leverage it.*

---

## 8. Suggested TODO / Improvements  

1. **Data‑type refinement** – Change columns that represent dates (`prediction_for_date`, `pack_start_date`, `pack_end_date`, `prediction_month`) to `DATE` or `TIMESTAMP`, and numeric fields (`data_bundle_in_mb`, `average_monthly_usage`, `daily_average_usage`, `predicted_data_balance_days`) to `BIGINT`/`DOUBLE`. This will improve storage efficiency and query performance.  

2. **Externalize the HDFS base path** – Replace the hard‑coded `LOCATION` with a variable sourced from a configuration file or Hive property (e.g., `${mnaas.base.path}`). This makes the DDL reusable across environments and eases migration if the namenode address changes.  

*Both changes can be applied via an ALTER TABLE statement after the initial creation, or by recreating the table with the updated DDL.*