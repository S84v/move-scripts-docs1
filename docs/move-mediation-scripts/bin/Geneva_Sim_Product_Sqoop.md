**Geneva_Sim_Product_Sqoop.sh – High‑Level Documentation**  

---

### 1. Purpose (one‑paragraph summary)  
`Geneva_Sim_Product_Sqoop.sh` is a production‑grade Bash driver that extracts the *Geneva Simulation Product* data set from an Oracle source, stages it in HDFS via Sqoop, and refreshes the corresponding Hive/Impala tables used by downstream analytics (e.g., Tableau dashboards). It performs housekeeping (HDFS cleanup, Hive truncation), loads the raw data, runs a post‑load “split” transformation, and emits detailed log entries. On failure it records the error and notifies the Move team by e‑mail.

---

### 2. Key Components & Responsibilities  

| Component | Responsibility |
|-----------|-----------------|
| **`. /app/hadoop_users/MNAAS/MNAAS_Property_Files/Geneva_Sim_Product_Sqoop.properties`** | Loads all environment‑specific variables (DB credentials, table names, HDFS paths, Impala host, mail lists, etc.). |
| **`logger` statements** | Writes timestamped messages to the script‑specific log file (`$Geneva_Sim_Product_SqoopLog`). |
| **`hadoop fs -rm -r $Geneva_Sim_Product_SqoopDir/*`** | Clears the target HDFS staging directory before each run. |
| **`sqoop import … --query "${Geneva_Sim_Product_SqoopQuery}"`** | Pulls data from Oracle into the staging directory, using a custom SQL query defined in the properties file. |
| **`hive -e "truncate table $dbname.$gen_sim_product_tblname"`** | Empties the target Hive table to guarantee idempotent loads. |
| **`impala-shell -i $IMPALAD_HOST -q "refresh …"`** | Forces Impala metadata refresh after Hive changes. |
| **`hive -e "load data inpath … into table …"`** | Moves the Sqoop‑generated files from HDFS into the Hive table. |
| **`hive -e "${Geneva_Sim_Product_SplitQuery}"`** | Executes a second SQL statement (usually an INSERT…SELECT) that populates a “split” table used for downstream reporting. |
| **Error handling block (`if [ $? -eq 0 ]; then … else … fi`)** | Detects Sqoop exit status, logs success/failure, and on failure sends an e‑mail alert with the log path. |
| **`mail` command** | Sends a formatted notification to the distribution list (`$ccList`, `$GTPMailId`). |

---

### 3. Inputs, Outputs & Side Effects  

| Category | Details |
|----------|---------|
| **Inputs (via properties file)** | `dbname`, `gen_sim_product_tblname`, `gen_sim_product_split_tblname`, `Geneva_Sim_Product_SqoopDir`, `Geneva_Sim_Product_SqoopLog`, Oracle connection vars (`ora_serverNameMOVE`, `ora_portNumberMOVE`, `ora_serviceNameMOVE`, `ora_usernameMOVE`, `ora_passwordMOVE`), `Geneva_Sim_Product_SqoopQuery`, `Geneva_Sim_Product_SplitQuery`, `IMPALAD_HOST`, `ccList`, `GTPMailId`. |
| **External services** | Oracle DB (source), Hadoop HDFS, Hive Metastore, Impala daemon, local mail transfer agent (SMTP). |
| **Outputs** | 1) HDFS files under `$Geneva_Sim_Product_SqoopDir` (temporary staging). 2) Populated Hive tables `$dbname.$gen_sim_product_tblname` and `$dbname.$gen_sim_product_split_tblname`. 3) Log file `$Geneva_Sim_Product_SqoopLog`. 4) Optional e‑mail alert on failure. |
| **Side effects** | - Deletes any pre‑existing files in the staging directory. - Truncates Hive tables (data loss if run against wrong DB). - Refreshes Impala metadata (may cause brief query stalls). |
| **Assumptions** | - The properties file exists and contains valid values. - Oracle credentials have SELECT rights on the queried objects. - Hive/Impala are reachable from the host running the script. - Sufficient HDFS quota for the import. - `mail` command is configured to send external e‑mail. |

---

### 4. Integration Points (how this script fits in the overall Move ecosystem)  

| Connection | Description |
|------------|-------------|
| **Move‑MSPS Tableau stack** | The Hive tables loaded here (`gen_sim_product_tblname` and its split counterpart) are consumed by Tableau dashboards generated by the `move‑MSPS` Tableau scripts (`tableauExecutionQueue.js`, etc.). |
| **Global utilities** | Although this script uses raw Bash commands, it follows the same naming conventions and logging pattern as other Move scripts (e.g., `move‑mediation‑scripts/bin/Dimension.sh`). |
| **Downstream batch jobs** | Subsequent nightly or hourly jobs that aggregate or export the simulation product data will read from the same Hive tables; they typically start after this script’s log entry “table loaded”. |
| **Alerting/ops monitoring** | The e‑mail sent on failure integrates with the Move team’s incident management system; the log file path is also monitored by the central log‑aggregation platform (e.g., Splunk). |
| **Potential upstream** | A separate Oracle extraction script may populate a staging view that `Geneva_Sim_Product_SqoopQuery` references. Changes to that view affect this script’s output. |

---

### 5. Operational Risks & Recommended Mitigations  

| Risk | Impact | Mitigation |
|------|--------|------------|
| **Incorrect properties file** (wrong DB, table names, or credentials) | Data loss (truncation of the wrong Hive table) or failed import. | Validate property values with a pre‑run sanity check script; keep a version‑controlled copy of the properties file. |
| **Sqoop import failure due to schema change** | Partial load, downstream jobs see stale data. | Add schema‑validation step (e.g., compare column count/types) before import; version the query in the properties file. |
| **HDFS directory removal accident** (path typo) | Deletion of unrelated data. | Use a whitelist of allowed directories; log the exact `rm -r` command before execution. |
| **Impala refresh latency** | Queries may return empty results briefly. | Schedule a short “quiet window” after refresh or use `invalidate metadata` only when necessary. |
| **Mail delivery failure** | No alert reaches ops team. | Verify SMTP connectivity; duplicate alert via pager or monitoring webhook. |
| **Hard‑coded `-m 1` (single mapper)** | Performance bottleneck for large data sets. | Tune mapper count based on data volume; expose as a configurable property. |

---

### 6. Running & Debugging the Script  

1. **Prerequisite check**  
   ```bash
   ls -l /app/hadoop_users/MNAAS/MNAAS_Property_Files/Geneva_Sim_Product_Sqoop.properties
   ```
   Ensure the file is readable and contains all required variables.

2. **Dry‑run (no data movement)**  
   ```bash
   export DRY_RUN=1   # (add a conditional in the script if needed)
   ./Geneva_Sim_Product_Sqoop.sh
   ```
   Or comment out the `sqoop import` line and run to verify HDFS cleanup and Hive commands.

3. **Execute** (as the Hadoop user that owns the HDFS directory)  
   ```bash
   ./Geneva_Sim_Product_Sqoop.sh > /tmp/run.out 2>&1
   tail -f $Geneva_Sim_Product_SqoopLog
   ```

4. **Debugging tips**  
   - Check the exit code of each command (`echo $?`).  
   - Verify HDFS content after `sqoop import`: `hadoop fs -ls $Geneva_Sim_Product_SqoopDir`.  
   - Validate Hive table row count: `hive -e "select count(*) from $dbname.$gen_sim_product_tblname;"`.  
   - Look for Impala refresh errors in the log; run the same `impala-shell` query manually.  
   - If the e‑mail is not sent, test the `mail` command alone: `echo test | mail -s "test" user@example.com`.

---

### 7. External Configuration & Environment Variables  

| Variable (populated from properties) | Role |
|--------------------------------------|------|
| `dbname` | Hive database name. |
| `gen_sim_product_tblname` | Target Hive table for raw import. |
| `gen_sim_product_split_tblname` | Target Hive table for split/derived data. |
| `Geneva_Sim_Product_SqoopDir` | HDFS staging directory. |
| `Geneva_Sim_Product_SqoopLog` | Full path to the script‑specific log file. |
| `ora_serverNameMOVE`, `ora_portNumberMOVE`, `ora_serviceNameMOVE`, `ora_usernameMOVE`, `ora_passwordMOVE` | Oracle connection details (production). |
| `Geneva_Sim_Product_SqoopQuery` | SQL SELECT statement passed to Sqoop (`--query`). |
| `IMPALAD_HOST` | Hostname/IP of the Impala daemon used for metadata refresh. |
| `Geneva_Sim_Product_SplitQuery` | Hive/Impala SQL that populates the split table. |
| `ccList`, `GTPMailId` | Email recipients for failure notifications. |
| `ccList` may be a comma‑separated list; `GTPMailId` is the primary recipient. |

If any of these variables are missing or empty, the script will fail early (e.g., Sqoop will abort with a connection error).

---

### 8. Suggested Improvements (TODO)  

1. **Add a pre‑execution validation block** – verify that all required variables are set, that the Oracle connection succeeds (`sqlplus -s` test), and that the HDFS target directory exists before attempting deletion.  
2. **Parameterize the mapper count** – expose `-m <num>` as a property (`SQOOP_MAPPERS`) to allow scaling based on data volume without editing the script.  

--- 

*End of documentation.*