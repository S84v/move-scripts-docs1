**File:** `move-mediation-scripts\bin\api_med_data_oracle_loading.sh`  

---

## 1. Purpose (One‑paragraph summary)

This script exports the Hive‑generated `API_MED_DATA` dataset from HDFS to the Oracle `API_MED_DATA` table using Sqoop. It first truncates the target Oracle table, then runs a single‑mapper Sqoop export, updates a shared process‑status file, logs progress to a dedicated log file, and sends an email/SDP ticket on failure. The script is designed to be invoked by a daily cron job and includes safeguards against concurrent executions.

---

## 2. Key Functions / Logical Units

| Function / Section | Responsibility |
|--------------------|----------------|
| **source properties** (`. /app/hadoop_users/MNAAS/.../MNAAS_api_med_data.properties`) | Loads all environment‑specific variables (DB credentials, file paths, status‑file name, log name, mail settings, etc.). |
| **api_med_data_table_Sqoop** | *Core export routine* – updates status flag, logs start, truncates Oracle table via `sqlplus`, runs `sqoop export`, logs success/failure, and calls termination routine on error. |
| **terminateCron_successful_completion** | Resets status flags to “idle”, records successful run time, writes final log entries, and exits with status 0. |
| **terminateCron_Unsuccessful_completion** | Records failure time, logs the error, and exits with status 1 (does **not** automatically send email – that is delegated to `email_and_SDP_ticket_triggering_step`). |
| **email_and_SDP_ticket_triggering_step** | Checks whether an alert has already been raised; if not, composes and sends an email to the configured distribution list and marks the alert as sent in the status file. |
| **Main program** | Prevents parallel runs by checking the PID stored in the status file, updates the PID, validates the daily‑process flag, invokes the export routine, and finally calls the appropriate termination function. |

---

## 3. Inputs, Outputs & Side‑Effects

| Category | Details |
|----------|---------|
| **Configuration / Env variables** (populated by the sourced `.properties` file) | `api_med_data_table_SqoopLogName`, `api_med_data_Sqoop_ProcessStatusFile`, `dbname`, `api_med_data_table_name`, `OrgDetails_ServerName`, `OrgDetails_PortNumber`, `OrgDetails_Service`, `OrgDetails_Username`, `OrgDetails_Password`, `MNAAS_Sqoop_api_med_data_Scriptname`, `ccList`, `GTPMailId`, `MNAAS_FlagValue` etc. |
| **External services** | Oracle database (`comprd` service) accessed via `sqlplus` and JDBC; Hadoop HDFS path `hdfs://NN-HA1/user/hive/warehouse/mnaas.db/api_med_data`; Sqoop binary; system logger (`logger`); local mail agent (`mail`). |
| **Primary input data** | Files in HDFS directory `.../api_med_data` (generated by upstream Hive/ETL jobs). |
| **Primary output** | Rows inserted into Oracle table `API_MED_DATA`. |
| **Log output** | Append‑only log file referenced by `$api_med_data_table_SqoopLogName`. |
| **Status file side‑effects** | The file `$api_med_data_Sqoop_ProcessStatusFile` is edited repeatedly to reflect: process flag, process name, job status, run time, PID, and whether an alert email has been sent. |
| **Email/SDP side‑effects** | On failure (first occurrence) an email is sent to `$GTPMailId` (CC `$ccList`). The script mentions “SDP ticket” but only logs that it was created; actual ticket creation is external to this script. |
| **Assumptions** | • The properties file exists and defines all required variables.<br>• Oracle client (`$ORACLE_HOME`) is correctly installed and reachable.<br>• The HDFS directory contains data in the expected `;`‑delimited format.<br>• The script runs with a user that has permission to read/write the status file, write the log, execute `sqlplus`, `sqoop`, and send mail. |

---

## 4. Interaction with Other Scripts / Components

| Connected Component | How the connection is made |
|---------------------|----------------------------|
| **Upstream Hive/ETL jobs** (e.g., `api_med_data` Hive table creation) | Produce the HDFS directory `.../api_med_data` that this script reads. Typically invoked by a preceding cron or Oozie workflow. |
| **`MNAAS_api_med_data.properties`** | Centralised configuration shared across all `api_med_data` scripts (e.g., `api_med_data_Sqoop_ProcessStatusFile`, log name, DB credentials). |
| **`truncate_table_api_med_data.sh`** (called via `sqlplus`) | SQL script that issues `TRUNCATE TABLE API_MED_DATA;` before the export. |
| **Other “api_med_data” scripts** (e.g., `api_med_data.sh`) | May orchestrate the full pipeline (data extraction → transformation → load). This script is the final load step. |
| **Cron scheduler** | The script is typically scheduled (e.g., daily) via a crontab entry that sets the appropriate environment before execution. |
| **Monitoring / Alerting system** | Relies on the log file and the email sent from `email_and_SDP_ticket_triggering_step`. External ticketing (SDP) is referenced but not directly invoked. |
| **Process‑status file** (`$api_med_data_Sqoop_ProcessStatusFile`) | Shared with other scripts to coordinate run windows, avoid concurrency, and expose job status to dashboards or downstream processes. |

---

## 5. Operational Risks & Recommended Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| **Concurrent execution** – PID check may fail if the status file is corrupted or the previous process hung. | Duplicate loads, data corruption, Oracle lock contention. | Add a timeout check on the stored PID (e.g., verify process still alive for > X minutes) and purge stale PID entries. |
| **Hard‑coded single mapper (`-m 1`)** – limits throughput and may cause long runtimes on large data sets. | SLA breach, resource under‑utilisation. | Evaluate data volume; increase mapper count and tune `--direct` mode if supported by Oracle. |
| **Plain‑text Oracle credentials** in the properties file. | Security breach. | Move credentials to a secure vault (e.g., HashiCorp Vault, OS‑level keyring) and source them at runtime. |
| **No error handling for `sqlplus` truncate step** – failure is ignored. | Stale data may remain, leading to duplicate rows after export. | Capture `sqlplus` exit code; abort if non‑zero and trigger alert. |
| **Mail command assumes a working MTA** – if mail fails, no alert is raised. | Silent failures. | Check mail command exit status; fallback to writing to a separate alert file or push to a monitoring API. |
| **Log file growth** – script appends indefinitely. | Disk exhaustion. | Rotate logs via `logrotate` or implement size‑based truncation within the script. |
| **Hard‑coded HDFS URI** (`NN-HA1`) may change across environments. | Breaks in DR or new clusters. | Parameterise the HDFS namenode in the properties file. |

---

## 6. Running / Debugging the Script

1. **Prerequisites**  
   - Verify that `/app/hadoop_users/MNAAS/MNAAS_Property_Files/MNAAS_api_med_data.properties` exists and contains all required variables.  
   - Ensure Oracle client (`$ORACLE_HOME`) is installed and `$ORACLE_HOME/bin/sqlplus` is executable.  
   - Confirm the HDFS path `hdfs://NN-HA1/user/hive/warehouse/mnaas.db/api_med_data` contains the expected `;`‑delimited files.  
   - The executing user must have read/write access to the status file and log file locations.

2. **Manual execution** (for testing)  
   ```bash
   cd /path/to/move-mediation-scripts/bin
   ./api_med_data_oracle_loading.sh   # run under the same environment as cron
   ```
   - The script prints debug traces because `set -x` is enabled.  
   - Check the log file `$api_med_data_table_SqoopLogName` for detailed output.  

3. **Debugging steps**  
   - **Check PID logic**: `cat $api_med_data_Sqoop_ProcessStatusFile | grep MNAAS_Script_Process_Id` to see stored PID.  
   - **Force a fresh run**: Delete or reset the status file entries (`MNAAS_Daily_ProcessStatusFlag=0`, `MNAAS_Script_Process_Id=`).  
   - **Validate Oracle connectivity**: Run the truncate command manually:  
     ```bash
     $ORACLE_HOME/bin/sqlplus move/move123@comprd @/app/hadoop_users/MNAAS/MNAAS_CronFiles/truncate_table_api_med_data.sh
     ```  
   - **Test Sqoop export alone** (replace variables with concrete values) to isolate issues.  

4. **Cron integration**  
   - Typical crontab entry (example):  
     ```cron
     0 2 * * * /app/hadoop_users/MNAAS/MNAAS_Scripts/api_med_data_oracle_loading.sh >> /dev/null 2>&1
     ```  
   - Ensure the cron environment sources the same profile that defines `PATH`, `ORACLE_HOME`, etc., or explicitly source the properties file inside the script (already done).  

---

## 7. External Config / Environment Variables

| Variable (populated from properties) | Meaning / Usage |
|--------------------------------------|-----------------|
| `api_med_data_table_SqoopLogName` | Full path of the log file where all `logger` and script output is appended. |
| `api_med_data_Sqoop_ProcessStatusFile` | Shared status file that tracks flags, PID, job status, and alert flag. |
| `dbname` | Database identifier used in log messages (e.g., `MNAAS`). |
| `api_med_data_table_name` | Logical name of the table (used only for logging). |
| `OrgDetails_ServerName`, `OrgDetails_PortNumber`, `OrgDetails_Service`, `OrgDetails_Username`, `OrgDetails_Password` | Oracle JDBC connection details for Sqoop. |
| `MNAAS_Sqoop_api_med_data_Scriptname` | Script name used in status‑file entries and logs. |
| `ccList`, `GTPMailId` | Email recipients for failure alerts. |
| `MNAAS_FlagValue` (derived from `MNAAS_Daily_ProcessStatusFlag`) | Controls whether the export is allowed to run (0 or 1). |
| `ORACLE_HOME` (hard‑coded) | Path to Oracle client binaries. |

If any of these variables are missing or empty, the script will fail early (e.g., `sqlplus` cannot connect, `sqoop` cannot resolve the JDBC URL).

---

## 8. Suggested Improvements (TODO)

1. **Add robust error handling for the truncate step** – capture `sqlplus` exit code, log failure, and invoke `email_and_SDP_ticket_triggering_step` before exiting.  
2. **Externalise sensitive credentials** – replace plain‑text Oracle username/password in the properties file with a secure vault lookup (e.g., `aws secretsmanager`, `HashiCorp Vault`) and inject them at runtime.

---