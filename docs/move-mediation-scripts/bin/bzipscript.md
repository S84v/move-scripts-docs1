**File:** `move-mediation-scripts\bin\bzipscript.sh`  

---

## 1. Purpose (One‑paragraph summary)

`bzipscript.sh` is a lightweight housekeeping utility that runs as part of the GTP (Global Telephony Platform) data‑move pipeline. It scans a fixed backup directory for GTP “IDS2” files matching the pattern `*22-03-1*.gtpc` (i.e., files for a specific day/month), logs whether any such files exist, and, if they do, compresses them with `gzip`. The script writes operational messages both to the system logger (`logger`) and to a daily log file under `/app/hadoop_users/mkoripal/GTPHadoopmoveLog`. The resulting `.gtpc.gz` archives are then ready for downstream ingestion (e.g., Sqoop/Hadoop loading scripts) that expect compressed input.

---

## 2. Key Elements

| Element | Type | Responsibility |
|---------|------|-----------------|
| `PGWHadoopmoveLog` | Variable | Base path for the daily log file (`/app/hadoop_users/mkoripal/GTPHadoopmoveLog`). |
| `gtp_hadoop_move()` | Function | Core logic: count matching files, log count, gzip files if present, log completion. |
| `count` | Local variable | Holds the number of files matching the pattern. |
| `logger` | External command | Sends timestamped messages to syslog **and** appends them to the daily log file. |
| `gzip` | External command | Compresses each matching `.gtpc` file in‑place (original file is replaced by `.gtpc.gz`). |
| `set -x` | Shell option | Enables command‑trace debugging for every executed line (useful in production logs). |
| Invocation (`gtp_hadoop_move`) | Statement | Executes the function when the script is run. |

---

## 3. Inputs, Outputs, Side‑effects & Assumptions

| Category | Details |
|----------|---------|
| **Inputs** | Files on the local filesystem: `/backup/GTP/IDS2_backup_04042022/*22-03-1*.gtpc`. The script assumes the directory exists and is readable. |
| **Outputs** | 1. Compressed files: each matching `.gtpc` becomes `.gtpc.gz` (original file is removed). <br>2. Log entries: <br> • Syslog (via `logger -s`). <br> • Daily log file: `$PGWHadoopmoveLog$(date +_%F)`. |
| **Side‑effects** | - Modification of the source directory (original files are overwritten by gzip). <br>- Growth of the daily log file. |
| **External Services / Resources** | - System logger (syslog daemon). <br>- Local filesystem (read/write). |
| **Assumptions** | - The pattern `*22-03-1*.gtpc` uniquely identifies the target day (hard‑coded date fragment). <br>- No other process is writing to the same files while this script runs. <br>- Sufficient disk space for the temporary gzip buffer. <br>- The user executing the script has permission to read the source files and write to the log directory. |

---

## 4. Integration Points (How it connects to other scripts/components)

| Connected Component | Relationship |
|---------------------|--------------|
| **Downstream ingestion scripts** (e.g., `api_med_data.sh`, `Sim_Inventory_Sqoop.sh`, `Tableau_security_Sqoop.sh`) | Expect the GTP backup files to be present in `/backup/GTP/IDS2_backup_04042022/` **compressed**. They likely invoke `bzipscript.sh` (directly or via a scheduler) before launching Sqoop/Hadoop loads. |
| **Scheduler / Cron** | Typically scheduled nightly (or hourly) to run after the backup process finishes, ensuring the files are ready for the next stage of the pipeline. |
| **Monitoring / Alerting** | Syslog entries generated by `logger` can be captured by centralized monitoring (e.g., Splunk, ELK) to verify that compression succeeded. |
| **Log rotation** | The daily log file path (`$PGWHadoopmoveLog$(date +_%F)`) is consumed by the system’s log‑rotation policy; other scripts may read the same log for audit purposes. |
| **Environment / Config** | The script defines `PGWHadoopmoveLog` internally; however, other scripts may export the same variable to keep a consistent log location across the pipeline. |

---

## 5. Operational Risks & Recommended Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| **Loss of original `.gtpc` files** – `gzip` replaces the source file. | Downstream jobs that expect the uncompressed name will fail. | Add `gzip -k` (keep original) or copy files to a staging area before compression. |
| **Race condition with concurrent writers** – Files may still be being written when the script runs. | Partial or corrupted archives. | Use a lock file (`flock`) or check file age (`find -mmin +5`) before compressing. |
| **Incorrect numeric comparison** – `[ $count == 0 ]` is a string comparison; may misbehave on some shells. | Unexpected branch execution. | Replace with `[ "$count" -eq 0 ]`. |
| **Unbounded log growth** – Daily log files are appended without rotation. | Disk exhaustion. | Integrate with logrotate or truncate after a size threshold. |
| **Hard‑coded date pattern** – The pattern `*22-03-1*` ties the script to a specific day/month. | Requires manual update each month. | Parameterize the date (e.g., `$(date +%y-%m-%d)`) or read from a config file. |
| **Missing directory or permission errors** – No explicit error handling. | Silent failures; downstream jobs receive no data. | Add checks for directory existence and exit codes; log errors explicitly. |

---

## 6. Running / Debugging the Script

1. **Standard execution (as scheduled):**  
   ```bash
   /opt/move-mediation-scripts/bin/bzipscript.sh
   ```
   The script will emit a trace (`+` lines) because of `set -x`, and all messages will appear in syslog and the daily log file.

2. **Manual test (dry‑run):**  
   - Comment out the `gzip` line or replace it with `echo "Would gzip $file"` to verify file selection without modifying data.  
   - Run with `bash -x bzipscript.sh` to see expanded commands.

3. **Debugging steps:**  
   - Verify the count: `ls /backup/GTP/IDS2_backup_04042022/*22-03-1*.gtpc | wc -l`.  
   - Check the log file: `tail -f /app/hadoop_users/mkoripal/GTPHadoopmoveLog_$(date +%F)`.  
   - Confirm compression: `ls -l /backup/GTP/IDS2_backup_04042022/*.gtpc.gz`.  

4. **Permissions check:**  
   ```bash
   sudo -u <run_user> test -r /backup/GTP/IDS2_backup_04042022 && \
   sudo -u <run_user> test -w /app/hadoop_users/mkoripal
   ```

---

## 7. External Configuration / Environment Variables

| Variable | Definition in Script | Expected Source |
|----------|----------------------|-----------------|
| `PGWHadoopmoveLog` | Hard‑coded to `/app/hadoop_users/mkoripal/GTPHadoopmoveLog` | Could be overridden by exporting the variable before invoking the script, but the script currently overwrites it. |
| `PATH`, `LOGGER` | Implicitly used by `logger` and `gzip`. | Assumed to be present in the standard system PATH. |
| `date` command | Used for timestamps and log‑file suffix. | System utility; locale may affect format. |

If the organization standardizes log locations via an environment file (e.g., `/etc/move_env.sh`), consider sourcing it at the top of the script: `source /etc/move_env.sh`.

---

## 8. Suggested Improvements (TODO)

1. **Add robust error handling and proper numeric comparison**  
   ```bash
   count=$(find /backup/GTP/IDS2_backup_04042022/ -maxdepth 1 -name '*22-03-1*.gtpc' | wc -l)
   if [ "$count" -eq 0 ]; then
       logger -s "... no files ..."
   else
       gzip -k /backup/GTP/IDS2_backup_04042022/*22-03-1*.gtpc
       logger -s "... zipped ..."
   fi
   ```

2. **Parameterize the date pattern and protect against concurrent execution**  
   - Accept a date argument (`$1`) or read from a config file.  
   - Use `flock -n /var/run/bzipscript.lock` to ensure only one instance runs at a time.  

Implementing these changes will make the script more maintainable, safer in production, and easier to integrate with the broader data‑move orchestration framework.