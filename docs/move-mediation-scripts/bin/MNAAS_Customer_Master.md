**File:** `move-mediation-scripts/bin/MNAAS_Customer_Master.sh`

---

## 1. Purpose (One‑paragraph Summary)

`MNAAS_Customer_Master.sh` extracts the **Customer Master** reference data from the Oracle GBS (Global Billing System) database, stages it in a dedicated HDFS directory via Sqoop, and refreshes the corresponding Hive/Impala table (`$dbname.$cust_master_tblname`). The script is part of the MNAAS (Mobile Network Analytics & Automation Suite) data‑move pipeline and supplies a clean, up‑to‑date customer dimension that downstream billing, reporting, and mediation jobs (e.g., `MNAAS_Billing_Export*` scripts) rely on.

---

## 2. Key Components & Responsibilities

| Component | Responsibility |
|-----------|-----------------|
| **Property source**<br>`. /app/hadoop_users/MNAAS/MNAAS_Property_Files/MNAAS_ShellScript.properties` | Loads all environment‑specific variables (DB credentials, HDFS paths, table names, mail lists, etc.). |
| **Logging** (`logger -s … >> $cust_master_SqoopLog…`) | Writes start, success, failure, and timing messages to a daily log file for audit and troubleshooting. |
| **HDFS cleanup** (`hadoop fs -rm -r $cust_HDFSSqoopDir/*`) | Removes any stale files in the staging directory before a new Sqoop import. |
| **Sqoop import** (`sqoop import … --query "${cust_master_SqoopQuery}" …`) | Pulls the Customer Master rows from Oracle into `$cust_HDFSSqoopDir`. Uses a single mapper (`-m 1`) and appends to the target directory. |
| **Post‑import Hive/Impala actions** (truncate, load, refresh) | - Truncates the Hive table to remove previous data.<br>- Refreshes Impala metadata before and after the load.<br>- Loads the newly staged files into Hive (`LOAD DATA INPATH …`). |
| **Error handling & notification** | On Sqoop failure, logs the error, records end‑time, and sends an email alert to `$ccList` and `$GTPMailId`. |
| **Debug flag** (`set -x`) | Enables Bash command tracing for easier manual debugging. |

---

## 3. Inputs, Outputs & Side Effects

| Category | Details |
|----------|---------|
| **Inputs** | • Oracle connection parameters (`$ora_serverNameGBS`, `$ora_portNumberGBS`, `$ora_serviceNameGBS`, `$ora_usernameGBS`, `$ora_passwordGBS`).<br>• Sqoop query (`$cust_master_SqoopQuery`).<br>• HDFS staging directory (`$cust_HDFSSqoopDir`).<br>• Hive/Impala table name (`$dbname.$cust_master_tblname`). |
| **Outputs** | • HDFS files under `$cust_HDFSSqoopDir` (CSV/TSV format generated by Sqoop).<br>• Populated Hive table (`$dbname.$cust_master_tblname`).<br>• Daily log file `$cust_master_SqoopLog_YYYY-MM-DD`.<br>• Optional email alert on failure. |
| **Side Effects** | • Deletes *all* existing files in `$cust_HDFSSqoopDir` before import (potential data loss if import fails).<br>• Refreshes Impala metadata (affects any concurrent queries). |
| **Assumptions** | • Oracle JDBC driver is available on the node running the script.<br>• Hive and Impala services are reachable (`$IMPALAD_HOST`).<br>• The properties file contains valid, non‑expired credentials.<br>• Mail subsystem (`mail` command) is configured and can reach `$GTPMailId`. |
| **External Services** | Oracle DB, Hadoop HDFS, Hive Metastore, Impala Daemon, SMTP server (for `mail`). |

---

## 4. Integration Points (How This Script Connects to the Rest of the System)

| Dependent Component | Interaction |
|---------------------|-------------|
| **Downstream billing/export scripts** (`MNAAS_Billing_Export*.sh`, `MNAAS_Billing_Geneva.sh`, etc.) | Query the Hive table `$dbname.$cust_master_tblname` for customer reference data when building billing extracts. |
| **Data quality / trend monitoring scripts** (`MNAAS_CDRs_trend_check_mail.sh`) | May validate row counts or key fields against the freshly loaded customer master. |
| **Orchestration layer** (e.g., Oozie, Airflow, cron) | Schedules this script before any job that consumes the customer master; ensures proper ordering. |
| **Properties repository** (`MNAAS_ShellScript.properties`) | Centralised configuration shared across all MNAAS scripts; any change here propagates to this script. |
| **Alerting/ops dashboard** | Log file path (`$cust_master_SqoopLog*`) is typically monitored by Splunk/ELK; failure email triggers incident tickets. |

---

## 5. Operational Risks & Recommended Mitigations

| Risk | Mitigation |
|------|------------|
| **Oracle connectivity / credential expiry** | Store passwords in a secure vault (e.g., HashiCorp Vault) and inject at runtime; monitor DB connection health. |
| **Single‑mapper Sqoop import** → long runtime, possible timeout | Evaluate data volume; increase `-m` to >1 if the source table is large and the Oracle side can handle parallel reads. |
| **Staging directory purge before successful import** → data loss if import fails | Add a “pre‑import backup” step (e.g., rename dir with timestamp) and only delete after successful load. |
| **Impala refresh latency** causing stale reads for downstream jobs | Schedule a short pause (`sleep 30`) after refresh or use `invalidate metadata` if needed; monitor refresh duration. |
| **Email notification failure** (SMTP down) → silent failures | Add fallback logging to a central alerting system (e.g., PagerDuty) and verify mail command exit status. |
| **Hard‑coded log file naming** may cause log rotation issues | Rotate logs via logrotate or include a size‑based rotation in the script. |
| **Unencrypted passwords in properties file** | Move to encrypted property files or environment variables protected by OS permissions. |

---

## 6. Execution & Debugging Guide

1. **Prerequisites**  
   - Ensure the user running the script has read access to `/app/hadoop_users/MNAAS/MNAAS_Property_Files/MNAAS_ShellScript.properties`.  
   - Verify Hadoop, Hive, Impala, and Oracle client tools are on the `$PATH`.  
   - Confirm the SMTP service is reachable.

2. **Manual Run**  
   ```bash
   # Switch to the appropriate user (e.g., mnaas)
   su - mnaas
   # Export any required runtime overrides (optional)
   export IMPALAD_HOST=impala01.company.com
   # Execute the script
   /app/hadoop_users/MNAAS/move-mediation-scripts/bin/MNAAS_Customer_Master.sh
   ```

3. **Debug Mode**  
   - The script already contains `set -x`; the command trace will be printed to stdout and captured in the log file.  
   - To increase verbosity, temporarily add `export SQOOP_DEBUG=true` before the Sqoop command.

4. **Log Inspection**  
   - Daily log location: `$cust_master_SqoopLog_$(date +%F)` (e.g., `/var/log/mnaas/cust_master_2025-12-04.log`).  
   - Look for lines containing “started”, “finished”, “table loaded”, or “sqoop process failed”.

5. **Post‑mortem on Failure**  
   - Check the exit code of the Sqoop command (`echo $?`).  
   - Verify Oracle connectivity (`tnsping $ora_serverNameGBS`).  
   - Inspect HDFS directory permissions (`hadoop fs -ls $cust_HDFSSqoopDir`).  
   - Review the email sent (if any) for additional context.

6. **Automation**  
   - Typically scheduled via cron or an Oozie workflow with a daily frequency, **after** any upstream data‑load jobs that might affect the customer master source.

---

## 7. External Configuration & Environment Variables

| Variable (defined in properties) | Role |
|----------------------------------|------|
| `dbname` | Hive database name. |
| `cust_master_tblname` | Target Hive/Impala table for customer master. |
| `cust_master_SqoopLog` | Base path/name for the daily Sqoop log file. |
| `cust_HDFSSqoopDir` | HDFS staging directory for Sqoop output. |
| `ora_serverNameGBS`, `ora_portNumberGBS`, `ora_serviceNameGBS` | Oracle connection details. |
| `ora_usernameGBS`, `ora_passwordGBS` | Oracle credentials. |
| `cust_master_SqoopQuery` | Full SELECT statement (with `$CONDITIONS`) used by Sqoop. |
| `IMPALAD_HOST` | Hostname/IP of the Impala daemon for metadata refresh. |
| `ccList` | Comma‑separated list of CC recipients for failure alerts. |
| `GTPMailId` | Primary recipient for failure alerts. |

*If any of these variables are missing or empty, the script will abort with a non‑zero exit status.*

---

## 8. Suggested Improvements (TODO)

1. **Add a checkpoint / rollback mechanism** – before deleting `$cust_HDFSSqoopDir`, move its current contents to a timestamped backup directory; on Sqoop failure, restore the backup to avoid data loss.

2. **Secure credential handling** – replace plain‑text Oracle password in the properties file with a call to a secret manager (e.g., `vault read -field=password secret/gbss/oracle`) and inject the password at runtime.

---